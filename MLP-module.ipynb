{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from src.DataSource import DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/titanic_train.csv\"\n",
    "ds = DataSource(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_load_split(target=['Survived'],\n",
    "                   ignore=['Name', 'Cabin', 'Ticket'])\n",
    "ds.define_problem()\n",
    "ds.train_val_split(ratio=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Binary'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_preprocess(ds.X_train, ds.y_train, train_set=True)\n",
    "ds.data_preprocess(ds.X_val, ds.y_val, train_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_mlp(X, y, problem, hidden_layer=1, unit=16):\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    tf.keras.backend.clear_session()  # clear graph session\n",
    "    model = keras.Sequential()\n",
    "    # input layer\n",
    "    model.add(keras.layers.Input(shape=(X.shape[1],)))\n",
    "    # hidden layer\n",
    "    for _ in range(hidden_layer):\n",
    "        model.add(keras.layers.Dense(unit, activation='relu'))\n",
    "    # output layer\n",
    "    if problem == \"Regression\":\n",
    "        model.add(keras.layers.Dense(1))\n",
    "    elif problem == \"Binary\":\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(keras.layers.Dense(y.shape[1], activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_mlps(X, y, problem, max_hidden_layers=1, units=[16], use_all=False):\n",
    "    if use_all:\n",
    "        max_hidden_layers = 3\n",
    "        units = [16, 32, 64, 128, 256]\n",
    "    else:\n",
    "        max_hidden_layers = max_hidden_layers\n",
    "        units = units\n",
    "        \n",
    "    structure_grid = [np.arange(max_hidden_layers)+1, units]\n",
    "    structured_models = []\n",
    "    for param_tuple in itertools.product(*structure_grid):\n",
    "        model = get_single_mlp(X, y, problem,\n",
    "                               hidden_layer=param_tuple[0],\n",
    "                               unit=param_tuple[1])\n",
    "        structured_models.append(model)\n",
    "        \n",
    "    return structured_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_single_mlp(ds.trans_X_train, ds.trans_y_train, ds.problem, hidden_layer=1, unit=16) #1\n",
    "model2 = get_single_mlp(ds.trans_X_train, ds.trans_y_train, ds.problem, hidden_layer=2, unit=16) #3\n",
    "model3 = get_single_mlp(ds.trans_X_train, ds.trans_y_train, ds.problem, hidden_layer=2, unit=32) #4\n",
    "models = get_mlps(ds.trans_X_train, ds.trans_y_train, ds.problem, max_hidden_layers=2, units=[16, 32])\n",
    "model_list = [model1, model2, model3]\n",
    "models_1 = model_list + models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### default compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7952\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4151 - accuracy: 0.8381\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8143\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7952\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.8048\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8381\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8143\n"
     ]
    }
   ],
   "source": [
    "# if model1, model2 results are same with models(1, 2)\n",
    "# get_single_mlp & get_mlps code above are okay\n",
    "# model order: 1, 3, 4, 1, 2, 3, 4\n",
    "for model in models_1:\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss=keras.losses.binary_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(x=ds.trans_X_train, y=ds.trans_y_train,\n",
    "          batch_size=64, epochs=10,\n",
    "          verbose=0, callbacks=[callbacks],\n",
    "          validation_data=(ds.trans_X_val, ds.trans_y_val), shuffle=True)\n",
    "    \n",
    "    model.evaluate(ds.trans_X_val, ds.trans_y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when optimizer is set `just before` compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7952\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8381\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8143\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7952\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.8048\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8381\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8143\n"
     ]
    }
   ],
   "source": [
    "# if model1, model2 results are same with models(1, 2)\n",
    "# get_single_mlp & get_mlps code above are okay\n",
    "# model order: 1, 3, 4, 1, 2, 3, 4\n",
    "for model in models_1:\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt,\n",
    "              loss=keras.losses.binary_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(x=ds.trans_X_train, y=ds.trans_y_train,\n",
    "          batch_size=64, epochs=10,\n",
    "          verbose=0, callbacks=[callbacks],\n",
    "          validation_data=(ds.trans_X_val, ds.trans_y_val), shuffle=True)\n",
    "    \n",
    "    model.evaluate(ds.trans_X_val, ds.trans_y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when optimizer is `already set` before compile\n",
    "- it doesn't follow above two results\n",
    "- **Every optimizer should be set before compiling each model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7952\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.8143\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.8190\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.8190\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.8048\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.4477 - accuracy: 0.8048\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4716 - accuracy: 0.8286\n"
     ]
    }
   ],
   "source": [
    "# if model1, model2 results are same with models(1, 2)\n",
    "# get_single_mlp & get_mlps code above are okay\n",
    "# model order: 1, 3, 4, 1, 2, 3, 4\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "for model in models_1:\n",
    "    model.compile(optimizer=opt,\n",
    "              loss=keras.losses.binary_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(x=ds.trans_X_train, y=ds.trans_y_train,\n",
    "          batch_size=64, epochs=10,\n",
    "          verbose=0, callbacks=[callbacks],\n",
    "          validation_data=(ds.trans_X_val, ds.trans_y_val), shuffle=True)\n",
    "    \n",
    "    model.evaluate(ds.trans_X_val, ds.trans_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(problem, model, optimizer='adam', lr=0.01):\n",
    "    # automatically set loss and metrics according to problem\n",
    "    if problem == \"Regression\":\n",
    "        loss = keras.losses.MSE\n",
    "        metrics = ['MSE', 'MAE']\n",
    "    elif problem == \"Binary\":\n",
    "        loss = keras.losses.binary_crossentropy\n",
    "        metrics = ['accuracy']\n",
    "    else:\n",
    "        loss = keras.losses.categorical_crossentropy\n",
    "        metrics = ['accuracy']\n",
    "    \n",
    "    # match optimizer argument to optimizer class\n",
    "    optimizer_classes = {'adadelta': keras.optimizers.Adadelta, 'sgd': keras.optimizers.SGD,\n",
    "                         'adam': keras.optimizers.Adam, 'adagrad': keras.optimizers.Adagrad,\n",
    "                         'adamax': keras.optimizers.Adamax, 'rmsprop': keras.optimizers.RMSprop}\n",
    "    optimizer_class = optimizer_classes[optimizer]\n",
    "    \n",
    "    optimizer_info = {'optimizer': optimizer,\n",
    "                      'lr': lr}\n",
    "    \n",
    "    opt = optimizer_class(learning_rate=lr)\n",
    "    model.compile(optimizer=opt,\n",
    "                           loss=loss,\n",
    "                           metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When using `compile_model` function\n",
    "- result is same with above two results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7952\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8381\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8143\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7952\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.8048\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8381\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8143\n"
     ]
    }
   ],
   "source": [
    "# if model1, model2 results are same with models(1, 2)\n",
    "# get_single_mlp & get_mlps code above are okay\n",
    "for model in models_1:\n",
    "    compile_model(ds.problem, model, optimizer='adam', lr=0.01)\n",
    "    \n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(x=ds.trans_X_train, y=ds.trans_y_train,\n",
    "          batch_size=64, epochs=10,\n",
    "          verbose=0, callbacks=[callbacks],\n",
    "          validation_data=(ds.trans_X_val, ds.trans_y_val), shuffle=True)\n",
    "    \n",
    "    model.evaluate(ds.trans_X_val, ds.trans_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_mlp(X, y, problem, hidden_layer=1, unit=16):\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    tf.keras.backend.clear_session()  # clear graph session\n",
    "    model = keras.Sequential()\n",
    "    # input layer\n",
    "    model.add(keras.layers.Input(shape=(X.shape[1],)))\n",
    "    # hidden layer\n",
    "    for _ in range(hidden_layer):\n",
    "        model.add(keras.layers.Dense(unit, activation='relu'))\n",
    "    # output layer\n",
    "    if problem == \"Regression\":\n",
    "        model.add(keras.layers.Dense(1))\n",
    "    elif problem == \"Binary\":\n",
    "        model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(keras.layers.Dense(y.shape[1], activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_model(problem, model, optimizer='adam', lr=0.01):\n",
    "    # automatically set loss and metrics according to problem\n",
    "    if problem == \"Regression\":\n",
    "        loss = keras.losses.MSE\n",
    "        metrics = ['MSE', 'MAE']\n",
    "    elif problem == \"Binary\":\n",
    "        loss = keras.losses.binary_crossentropy\n",
    "        metrics = ['accuracy']\n",
    "    else:\n",
    "        loss = keras.losses.categorical_crossentropy\n",
    "        metrics = ['accuracy']\n",
    "    \n",
    "    # match optimizer argument to optimizer class\n",
    "    optimizer_classes = {'adadelta': keras.optimizers.Adadelta, 'sgd': keras.optimizers.SGD,\n",
    "                         'adam': keras.optimizers.Adam, 'adagrad': keras.optimizers.Adagrad,\n",
    "                         'adamax': keras.optimizers.Adamax, 'rmsprop': keras.optimizers.RMSprop}\n",
    "    optimizer_class = optimizer_classes[optimizer]\n",
    "    \n",
    "    optimizer_info = {'optimizer': optimizer,\n",
    "                      'lr': lr}\n",
    "    \n",
    "    opt = optimizer_class(learning_rate=lr)\n",
    "    model.compile(optimizer=opt,\n",
    "                           loss=loss,\n",
    "                           metrics=metrics)\n",
    "    \n",
    "def train_model(model, X_train, y_train, X_val, y_val,\n",
    "                batch_size=None, epochs=1, verbose=0, callbacks=None,\n",
    "                shuffle=True, steps_per_epoch=None):\n",
    "    # set callbacks; EarlyStopping\n",
    "    if callbacks:\n",
    "        callbacks = callbacks\n",
    "    else:\n",
    "        callbacks = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  patience=5,\n",
    "                                                  restore_best_weights=True)\n",
    "    \n",
    "    model.fit(x=X_train, y=y_train,\n",
    "              batch_size=batch_size, epochs=epochs,\n",
    "              verbose=verbose, callbacks=callbacks,\n",
    "              validation_data=(X_val, y_val), shuffle=shuffle)\n",
    "    \n",
    "    val_loss = model.evaluate(X_val, y_val, verbose=verbose)\n",
    "    print(\"{} model is trained. best val loss is: {}\".format(model.name, val_loss))\n",
    "    \n",
    "    return model, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need to define `auto_fit` function\n",
    "1. set the `param_grid`: (hidden_layers, units, optimizers, lrs)\n",
    "2. call whole functions making trained models according to the `param_grid`\n",
    "  - `get_single_mlp`\n",
    "  - `compile_model`\n",
    "  - `train_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_fit(problem, X_train, y_train, X_val, y_val,\n",
    "             hidden_layers=[1], units=[16],\n",
    "             optimizers=['adam'], lrs=[0.001],\n",
    "             batch_size=None, epochs=1, verbose=0,\n",
    "             callbacks=None, shuffle=True,\n",
    "             steps_per_epoch=None,\n",
    "             use_all=False):\n",
    "    \n",
    "    if use_all:\n",
    "        hidden_layers = [1, 2, 3]\n",
    "        units = [16, 32, 64, 128, 256]\n",
    "        optimizers = ['adam', 'adadelta', 'adamax', 'adagrad', 'sgd', 'rmsprop']\n",
    "        lrs = [0.001, 0.01, 0.02, 0.1]\n",
    "    else:\n",
    "        hidden_layers = hidden_layers\n",
    "        units = units\n",
    "        optimizers = optimizers\n",
    "        lrs = lrs\n",
    "        \n",
    "    models = []\n",
    "    val_losses = []\n",
    "    param_info = []\n",
    "    param_grid = [hidden_layers, units, optimizers, lrs]\n",
    "    for param_tuple in itertools.product(*param_grid):\n",
    "        hidden_layer = param_tuple[0]\n",
    "        unit = param_tuple[1]\n",
    "        optimizer = param_tuple[2]\n",
    "        lr = param_tuple[3]\n",
    "        \n",
    "        param_dict = {'hidden_layer': hidden_layer,\n",
    "                      'unit': unit,\n",
    "                      'optimizer': optimizer,\n",
    "                      'lr': lr}\n",
    "        \n",
    "        model= get_single_mlp(X_train, y_train, problem,\n",
    "                              hidden_layer=hidden_layer, unit=unit)\n",
    "        \n",
    "        compile_model(problem, model, optimizer=optimizer, lr=lr)\n",
    "        \n",
    "        model, val_loss = train_model(model, X_train, y_train, X_val, y_val,\n",
    "                                      batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
    "                                      callbacks=callbacks, shuffle=shuffle,\n",
    "                                      steps_per_epoch=steps_per_epoch)\n",
    "    \n",
    "        models.append(model)\n",
    "        val_losses.append(val_loss)\n",
    "        param_info.append(param_dict)\n",
    "        \n",
    "    return models, param_info, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ds.trans_X_train\n",
    "y_train = ds.trans_y_train\n",
    "X_val = ds.trans_X_val\n",
    "y_val = ds.trans_y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential model is trained. best val loss is: [0.45781596785499934, 0.7952380952380952]\n",
      "sequential model is trained. best val loss is: [0.4156928491024744, 0.8333333333333334]\n",
      "sequential model is trained. best val loss is: [0.7017272103400457, 0.46190476190476193]\n",
      "sequential model is trained. best val loss is: [0.6979270492281232, 0.4857142857142857]\n",
      "sequential model is trained. best val loss is: [0.4900653515543256, 0.8047619047619048]\n",
      "sequential model is trained. best val loss is: [0.4315620473452977, 0.819047619047619]\n",
      "sequential model is trained. best val loss is: [0.6954697807629903, 0.5571428571428572]\n",
      "sequential model is trained. best val loss is: [0.6903100428127107, 0.5904761904761905]\n",
      "sequential model is trained. best val loss is: [0.41507263211976914, 0.8380952380952381]\n",
      "sequential model is trained. best val loss is: [0.43994153227124894, 0.8380952380952381]\n",
      "sequential model is trained. best val loss is: [0.6860601101602827, 0.6]\n",
      "sequential model is trained. best val loss is: [0.6834122152555556, 0.6047619047619047]\n",
      "sequential model is trained. best val loss is: [0.4548098907584236, 0.8142857142857143]\n",
      "sequential model is trained. best val loss is: [0.43343487779299417, 0.8]\n",
      "sequential model is trained. best val loss is: [0.6919313947359721, 0.5952380952380952]\n",
      "sequential model is trained. best val loss is: [0.687483058656965, 0.6095238095238096]\n"
     ]
    }
   ],
   "source": [
    "models, param_info, val_losses = auto_fit(ds.problem, X_train, y_train, X_val, y_val,\n",
    "                                          hidden_layers=[1, 2], units=[16, 32],\n",
    "                                          optimizers=['adam', 'adadelta'], lrs=[0.01, 0.02],\n",
    "                                          batch_size=64, epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1, model_info = get_single_mlp(ds.trans_X_train, ds.trans_y_train, ds.problem, hidden_layer=1, unit=16) #1\n",
    "# model2, model_info = get_single_mlp(ds.trans_X_train, ds.trans_y_train, ds.problem, hidden_layer=2, unit=16) #3\n",
    "# model3, model_info = get_single_mlp(ds.trans_X_train, ds.trans_y_train, ds.problem, hidden_layer=2, unit=32) #4\n",
    "# models, models_info = get_mlps(ds.trans_X_train, ds.trans_y_train, ds.problem, max_hidden_layers=2, units=[16, 32])\n",
    "# model_list = [model1, model2, model3]\n",
    "# models_1 = model_list + models\n",
    "# hidden=1, unit=16, 'adam', lr=0.01  --  loss: 0.4578 - accuracy: 0.7952 \n",
    "# hidden=2, unit=16, 'adam', lr=0.01  --  loss: 0.4151 - accuracy: 0.8381\n",
    "# hidden=2, unit=32, 'adam', lr=0.01  --  loss: 0.4548 - accuracy: 0.8143\n",
    "# hidden=1, unit=16, 'adam', lr=0.01  --  loss: 0.4578 - accuracy: 0.7952\n",
    "# hidden=1, unit=32, 'adam', lr=0.01  --  loss: 0.4901 - accuracy: 0.8048\n",
    "# hidden=2, unit=16, 'adam', lr=0.01  --  loss: 0.4151 - accuracy: 0.8381\n",
    "# hidden=2, unit=32, 'adam', lr=0.01  --  loss: 0.4548 - accuracy: 0.8143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hidden_layer': 1, 'unit': 16, 'optimizer': 'adam', 'lr': 0.01},\n",
       " {'hidden_layer': 1, 'unit': 16, 'optimizer': 'adam', 'lr': 0.02},\n",
       " {'hidden_layer': 1, 'unit': 16, 'optimizer': 'adadelta', 'lr': 0.01},\n",
       " {'hidden_layer': 1, 'unit': 16, 'optimizer': 'adadelta', 'lr': 0.02},\n",
       " {'hidden_layer': 1, 'unit': 32, 'optimizer': 'adam', 'lr': 0.01},\n",
       " {'hidden_layer': 1, 'unit': 32, 'optimizer': 'adam', 'lr': 0.02},\n",
       " {'hidden_layer': 1, 'unit': 32, 'optimizer': 'adadelta', 'lr': 0.01},\n",
       " {'hidden_layer': 1, 'unit': 32, 'optimizer': 'adadelta', 'lr': 0.02},\n",
       " {'hidden_layer': 2, 'unit': 16, 'optimizer': 'adam', 'lr': 0.01},\n",
       " {'hidden_layer': 2, 'unit': 16, 'optimizer': 'adam', 'lr': 0.02},\n",
       " {'hidden_layer': 2, 'unit': 16, 'optimizer': 'adadelta', 'lr': 0.01},\n",
       " {'hidden_layer': 2, 'unit': 16, 'optimizer': 'adadelta', 'lr': 0.02},\n",
       " {'hidden_layer': 2, 'unit': 32, 'optimizer': 'adam', 'lr': 0.01},\n",
       " {'hidden_layer': 2, 'unit': 32, 'optimizer': 'adam', 'lr': 0.02},\n",
       " {'hidden_layer': 2, 'unit': 32, 'optimizer': 'adadelta', 'lr': 0.01},\n",
       " {'hidden_layer': 2, 'unit': 32, 'optimizer': 'adadelta', 'lr': 0.02}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, param_info, val_losses = auto_fit(ds.problem, X_train, y_train, X_val, y_val,\n",
    "                                          hidden_layers=[2], units=[32],\n",
    "                                          optimizers=['adam'], lrs=[0.02],\n",
    "                                          batch_size=64, epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
